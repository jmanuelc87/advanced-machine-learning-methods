{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Fully Connected Deep Neural Networks\n",
    "\n",
    "#### Activity 1b: Implementing a Fully Connected Network for Kaggle ASL Dataset\n",
    "\n",
    "- Objective\n",
    "\n",
    "The aim of this part of the activity is to apply your understanding of Fully Connected Networks by implementing a multilayer network for the [Kaggle ASL (American Sign Language) dataset](https://www.kaggle.com/datasets/grassknoted/asl-alphabet). While you have been provided with a complete solution for a Fully Connected Network using Numpy for the MNIST dataset, you are encouraged to try to come up with the solution.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams of 3 or 4 members. Submissions from smaller or larger teams will not be accepted unless prior approval has been granted (only due to exceptional circumstances). While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Load and Preprocess Data: You are provided a starter code to load the data. Be sure to understand the code.\n",
    "\n",
    "    Review MNIST Notebook (Optional): Before diving into this activity, you have the option to revisit the MNIST example to refresh your understanding of how to build a Fully Connected Network using Numpy.\n",
    "\n",
    "    Start Fresh: Although you can refer to the MNIST solution at any point, try to implement the network for the ASL dataset on your own. This will reinforce your learning and understanding of the architecture and mathematics involved.\n",
    "\n",
    "    Implement Forward and Backward Pass: Write the code to perform the forward and backward passes, keeping in mind the specific challenges and characteristics of the ASL dataset.\n",
    "    \n",
    "     Design the Network: Create the architecture of the Fully Connected Network tailored for the ASL dataset. Choose the number of hidden layers, neurons, and hyperparameters judiciously.\n",
    "\n",
    "    Train the Model: Execute the training loop, ensuring to track performance metrics such as loss and accuracy.\n",
    "\n",
    "    Analyze and Document: Use Markdown cells to document in detail the choices you made in terms of architecture and hyperparameters, you may use figures, equations, etc to aid in your explanations. Include any metrics that help justify these choices and discuss the model's performance.  \n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Appropriateness of chosen architecture and hyperparameters for the ASL dataset\n",
    "    - Performance of the model on the ASL dataset (at least 70% acc)\n",
    "    - Quality of Markdown documentation\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -q numpy\n",
    "#%pip install -q pandas\n",
    "#%pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files\n",
    "\n",
    "Load the csv files using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('asl_data/sign_mnist_train.csv')\n",
    "valid_df = pd.read_csv('asl_data/sign_mnist_valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10143</th>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>40</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>80</td>\n",
       "      <td>79</td>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>144</td>\n",
       "      <td>148</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23578</th>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "      <td>90</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>114</td>\n",
       "      <td>118</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>...</td>\n",
       "      <td>101</td>\n",
       "      <td>178</td>\n",
       "      <td>166</td>\n",
       "      <td>169</td>\n",
       "      <td>171</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8963</th>\n",
       "      <td>10</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>151</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>211</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>210</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25921</th>\n",
       "      <td>13</td>\n",
       "      <td>205</td>\n",
       "      <td>207</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>...</td>\n",
       "      <td>245</td>\n",
       "      <td>244</td>\n",
       "      <td>242</td>\n",
       "      <td>239</td>\n",
       "      <td>241</td>\n",
       "      <td>233</td>\n",
       "      <td>205</td>\n",
       "      <td>165</td>\n",
       "      <td>129</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18795</th>\n",
       "      <td>7</td>\n",
       "      <td>176</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>178</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>66</td>\n",
       "      <td>48</td>\n",
       "      <td>165</td>\n",
       "      <td>151</td>\n",
       "      <td>98</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>68</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7978</th>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>167</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>...</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>188</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>187</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>19</td>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>105</td>\n",
       "      <td>109</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>117</td>\n",
       "      <td>...</td>\n",
       "      <td>117</td>\n",
       "      <td>194</td>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>175</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23364</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>154</td>\n",
       "      <td>158</td>\n",
       "      <td>161</td>\n",
       "      <td>163</td>\n",
       "      <td>166</td>\n",
       "      <td>168</td>\n",
       "      <td>169</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>94</td>\n",
       "      <td>103</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>116</td>\n",
       "      <td>137</td>\n",
       "      <td>180</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5413</th>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>98</td>\n",
       "      <td>99</td>\n",
       "      <td>104</td>\n",
       "      <td>116</td>\n",
       "      <td>126</td>\n",
       "      <td>135</td>\n",
       "      <td>144</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>173</td>\n",
       "      <td>89</td>\n",
       "      <td>138</td>\n",
       "      <td>157</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21385</th>\n",
       "      <td>16</td>\n",
       "      <td>138</td>\n",
       "      <td>141</td>\n",
       "      <td>147</td>\n",
       "      <td>149</td>\n",
       "      <td>152</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>158</td>\n",
       "      <td>160</td>\n",
       "      <td>...</td>\n",
       "      <td>202</td>\n",
       "      <td>196</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>195</td>\n",
       "      <td>195</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "10143     15      45      42      38      36      37      38      40      42   \n",
       "23578     12      76      90      99     106     112     114     118     121   \n",
       "8963      10     149     149     151     152     153     153     154     155   \n",
       "25921     13     205     207     208     208     208     209     209     209   \n",
       "18795      7     176     177     178     178     178     177     177     177   \n",
       "7978       0     164     165     166     166     166     167     168     168   \n",
       "5530      19      87      92      94      99     105     109     113     115   \n",
       "23364      3     150     154     158     161     163     166     168     169   \n",
       "5413      15      96      98      99     104     116     126     135     144   \n",
       "21385     16     138     141     147     149     152     155     157     158   \n",
       "\n",
       "       pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "10143      41  ...        80        79        81        81        82   \n",
       "23578     123  ...       101       178       166       169       171   \n",
       "8963      155  ...       211       210       209       209       209   \n",
       "25921     208  ...       245       244       242       239       241   \n",
       "18795     177  ...       105        66        48       165       151   \n",
       "7978      168  ...       187       189       188       188       188   \n",
       "5530      117  ...       117       194       178       181       179   \n",
       "23364     171  ...        94       103       110       114       116   \n",
       "5413      151  ...       255       255       255       255       253   \n",
       "21385     160  ...       202       196       197       196       195   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "10143        83        65       144       148       117  \n",
       "23578       172       173       173       173       173  \n",
       "8963        210       209       209       208       206  \n",
       "25921       233       205       165       129       106  \n",
       "18795        98       107       116        68        59  \n",
       "7978        187       187       187       187       186  \n",
       "5530        178       175       173       173       173  \n",
       "23364       137       180       204       203       191  \n",
       "5413        173        89       138       157       142  \n",
       "21385       195       196       195       195       194  \n",
       "\n",
       "[10 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = train_df['label'].unique()\n",
    "\n",
    "sorted(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset\n",
    "\n",
    "Creates the validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_df['label'])\n",
    "y_val = np.array(valid_df['label'])\n",
    "del train_df['label']\n",
    "del valid_df['label']\n",
    "x_train = train_df.values.astype(np.float32)\n",
    "x_val = valid_df.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5379, 784), (1793, 784), (5379,), (1793,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, shuffle=True, random_state=43)\n",
    "\n",
    "x_val.shape, x_test.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a',\n",
       " 1: 'b',\n",
       " 2: 'c',\n",
       " 3: 'd',\n",
       " 4: 'e',\n",
       " 5: 'f',\n",
       " 6: 'g',\n",
       " 7: 'h',\n",
       " 8: 'i',\n",
       " 9: 'j',\n",
       " 10: 'k',\n",
       " 11: 'l',\n",
       " 12: 'm',\n",
       " 13: 'n',\n",
       " 14: 'o',\n",
       " 15: 'p',\n",
       " 16: 'q',\n",
       " 17: 'r',\n",
       " 18: 's',\n",
       " 19: 't',\n",
       " 20: 'u',\n",
       " 21: 'v',\n",
       " 22: 'w',\n",
       " 23: 'x',\n",
       " 24: 'y',\n",
       " 25: 'z'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create targets to labels\n",
    "\n",
    "alphabet=list(string.ascii_lowercase)\n",
    "\n",
    "target2label = { i: l for i,l in enumerate(alphabet) }\n",
    "\n",
    "target2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise\n",
    "\n",
    "Perform normalization on the dataset using the train mean and std. dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(x_mean, x_std, x_data):\n",
    "    return (x_data - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mean = x_train.mean()\n",
    "x_std = x_train.std()\n",
    "\n",
    "x_train = normalise(x_mean, x_std, x_train)\n",
    "x_val = normalise(x_mean, x_std, x_val)\n",
    "x_test = normalise(x_mean, x_std, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(np.reshape(image, (28, 28)), cmap=plt.get_cmap('gray'))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is labeled with the label: c\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rnd_idx = np.random.randint(len(y_test))\n",
    "print(f'The image is labeled with the label: {target2label[y_test[rnd_idx]]}')\n",
    "plot_image(x_test[rnd_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecuaciones para nuestro modelo\n",
    "\n",
    "\n",
    "$$z^1 = W^1 X + b^1$$\n",
    "\n",
    "$$a^1 = ReLU(z^1) $$\n",
    "\n",
    "$$z^2 = W^2 a^1 + b^2$$\n",
    "\n",
    "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
    "\n",
    "\n",
    "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones adicionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_minibatches(mb_size, x, y, shuffle = True):\n",
    "    \"\"\"This function creates mini-batches from the x and y datasets\n",
    "\n",
    "    Args:\n",
    "        mb_size (int): mini-batch size\n",
    "        x (np.ndarray): train, valid or test dataset\n",
    "        y (np.ndarray): train, valid or test labels\n",
    "        shuffle (bool, optional): specifies if the batches are shuffled. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: returns a tuple with the batches in pairs (x,y)\n",
    "    \"\"\"\n",
    "    assert x.shape[0] == y.shape[0], 'Error en cantidad de muestras'\n",
    "    total_data = x.shape[0]\n",
    "    if shuffle: \n",
    "        idxs = np.arange(total_data)\n",
    "        np.random.shuffle(idxs)\n",
    "        x = x[idxs]\n",
    "        y = y[idxs]  \n",
    "    return ((x[i:i+mb_size], y[i:i+mb_size]) for i in range(0, total_data, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuestra clase Linear, ReLU y Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class np_tensor(np.ndarray): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Clase Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, input_size: int, output_size: int) -> None:\n",
    "        \"\"\"Fully Connected Layer\n",
    "\n",
    "        Args:\n",
    "            input_size (int): input size of the fully connected layer\n",
    "            output_size (int): ouput size of the fully connected layer\n",
    "        \"\"\"\n",
    "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
    "        self.b = (np.zeros((output_size, 1))).view(np_tensor)\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        return self.W @ X + self.b\n",
    "    \n",
    "    def backward(self, X, Z):        \n",
    "        X.grad = self.W.T @ Z.grad\n",
    "        self.W.grad = Z.grad @ X.T\n",
    "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \n",
    "    def __call__(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    def backward(self, Z, A):\n",
    "        Z.grad = A.grad.copy()\n",
    "        Z.grad[Z <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "class Softmax:\n",
    "    \n",
    "    def __call__(self, z) -> Any:\n",
    "        exp = np.exp(z)\n",
    "        sum = np.sum(exp)\n",
    "        return np.divide(exp, sum).copy()\n",
    "    \n",
    "    def backward(self, S):\n",
    "        S_vector = S.reshape(S.shape[0],1)\n",
    "        S_matrix = np.tile(S_vector,S.shape[0])\n",
    "        S.grad = np.diag(S) - (S_matrix * np.transpose(S_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clase Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "\n",
    "    def __init__(self, layers = None) -> None:\n",
    "        \"\"\"Creates a Neural Network of stacked layers\n",
    "\n",
    "        Args:\n",
    "            layers (list, optional): The list of layers. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "        self.outputs = {}\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.outputs['l0'] = x\n",
    "        for i, layer in enumerate(self.layers, 1):\n",
    "            x = layer(x)\n",
    "            self.outputs['l' + str(i)] = x\n",
    "        return x\n",
    "\n",
    "    def backward(self):\n",
    "        for i in reversed(range(len(self.layers))):\n",
    "            li = self.outputs['l' + str(i)]\n",
    "            li1 = self.outputs['l' + str(i + 1)]\n",
    "            self.layers[i].backward(li, li1)\n",
    "\n",
    "    def update(self, learning_rate = 1e-3):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (ReLU, Softmax)): continue\n",
    "            layer.W = layer.W - learning_rate * layer.W.grad\n",
    "            layer.b = layer.b - learning_rate * layer.b.grad\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.argmax(self.__call__(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxXEntropy(x, y):\n",
    "    batch_size = x.shape[1]\n",
    "    exp_scores = np.exp(x)\n",
    "    probs = exp_scores / exp_scores.sum(axis=0)\n",
    "    preds = probs.copy()\n",
    "    \n",
    "    # Reshape de y para asegurar la forma correcta\n",
    "    y = y.reshape(1, -1)\n",
    "    \n",
    "    # Cost\n",
    "    y_hat = probs[y, np.arange(batch_size)]\n",
    "    cost = np.sum(-np.log(y_hat)) / batch_size\n",
    "    \n",
    "    # Gradient calculation\n",
    "    probs[y, np.arange(batch_size)] -= 1  # Corregido de y.squeeze() a y.reshape\n",
    "    x.grad = probs.copy()\n",
    "    \n",
    "    return preds, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epochs,mb_size=128, learning_rate=1e-3):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x,y) in enumerate(create_minibatches(mb_size, x_train,y_train)):\n",
    "            scores= model(x.T.view(np_tensor))\n",
    "            _, cost =softmaxXEntropy(scores,y)\n",
    "            model.backward()\n",
    "            model.update(learning_rate)\n",
    "        print(f'cost: {cost}, accuracy: {accuracy(x_val,y_val,mb_size)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(x, y, mb_size):\n",
    "    correct=0\n",
    "    total= 0\n",
    "    for i,(x,y) in enumerate(create_minibatches(mb_size,x,y)):\n",
    "        pred= model(x.T.view(np_tensor))\n",
    "        correct+= np.sum(np.argmax(pred,axis=0)== y.squeeze())\n",
    "        total+= pred.shape[1]\n",
    "    return correct/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "model = Sequential([Linear(784, 200), ReLU(), Linear(200, 200), ReLU(), Linear(200, num_classes)])\n",
    "mb_size=512\n",
    "learning_rate=1e-4\n",
    "epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost: 0.01266310302084125, accuracy: 0.7659416248373303\n",
      "cost: 0.013928209530629647, accuracy: 0.7700316043874326\n",
      "cost: 0.011181529558386659, accuracy: 0.7663134411600669\n",
      "cost: 0.012493259688780923, accuracy: 0.7687302472578547\n",
      "cost: 0.011427962550568374, accuracy: 0.7655698085145938\n",
      "cost: 0.011067607386784918, accuracy: 0.7685443390964863\n",
      "cost: 0.00938627117051705, accuracy: 0.769659788064696\n",
      "cost: 0.0086105773428329, accuracy: 0.7724484104852203\n",
      "cost: 0.009095824342744955, accuracy: 0.7698456962260644\n",
      "cost: 0.008870426081459321, accuracy: 0.7694738799033277\n",
      "cost: 0.008614438051964232, accuracy: 0.7705893288715374\n",
      "cost: 0.009224193810976805, accuracy: 0.7698456962260644\n",
      "cost: 0.007509742882495069, accuracy: 0.7687302472578547\n",
      "cost: 0.008289827693976369, accuracy: 0.7702175125488009\n",
      "cost: 0.008259664189772174, accuracy: 0.7679866146123815\n",
      "cost: 0.007530512429360846, accuracy: 0.772262502323852\n",
      "cost: 0.007510590337284543, accuracy: 0.7694738799033277\n",
      "cost: 0.0060921386856784155, accuracy: 0.7702175125488009\n",
      "cost: 0.005977910089707417, accuracy: 0.769659788064696\n",
      "cost: 0.005282452091714781, accuracy: 0.7707752370329057\n"
     ]
    }
   ],
   "source": [
    "train(model,epochs,mb_size,learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7830451756832125\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(x_test,y_test, mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your model on Random data from your test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el valor predicho es: v el valor real es:u\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(len(y_test))\n",
    "plot_image(x_test[idx].reshape(28,28))\n",
    "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
    "print(f'el valor predicho es: {alphabet[pred]} el valor real es:{alphabet[y_test[idx]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
